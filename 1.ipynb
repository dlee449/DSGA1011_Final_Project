{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "setup_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeb2362f-2c6f-4b1b-fd30-e09b258aec2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import base64\n",
        "import asyncio\n",
        "import time\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Callable\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import google.generativeai as genai\n",
        "\n",
        "ROOT_DRIVE_PATH = \"/content/drive/MyDrive/DSGA1011/Project\"\n",
        "drive.mount('/content/drive')\n",
        "SPATIAL_MM_ROOT = Path('/content/Spatial-MM')\n",
        "IMAGES_ROOT = Path(ROOT_DRIVE_PATH) / 'Spatial_MM-Benchmark'\n",
        "COT_IMAGE_DIR = IMAGES_ROOT / 'Spatial_MM_CoT'\n",
        "OUTPUT_DIR = Path(ROOT_DRIVE_PATH) / 'spatial_mm_outputs'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "if not SPATIAL_MM_ROOT.exists():\n",
        "    os.system(f'git clone https://github.com/FatemehShiri/Spatial-MM.git {SPATIAL_MM_ROOT}')\n",
        "\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading API keys: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prompts_models"
      },
      "outputs": [],
      "source": [
        "async def call_gemini_async(model_name, prompt, image_b64, max_tokens):\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "    img_bytes = base64.b64decode(image_b64[\"base64\"])\n",
        "    img = Image.open(BytesIO(img_bytes))\n",
        "\n",
        "    def _call():\n",
        "        return model.generate_content(\n",
        "            [prompt, img],\n",
        "            generation_config={\"max_output_tokens\": max_tokens, \"temperature\": 0}\n",
        "        ).text.strip()\n",
        "\n",
        "    loop = asyncio.get_running_loop()\n",
        "    return await loop.run_in_executor(None, _call)\n",
        "\n",
        "async def call_openai_async(model_name, prompt, image_b64, max_tokens):\n",
        "    image_url = f\"data:{image_b64['mime']};base64,{image_b64['base64']}\"\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"text\", \"text\": prompt},\n",
        "            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
        "        ]}\n",
        "    ]\n",
        "\n",
        "    if \"gpt5\" in model_name:\n",
        "        kwargs = {\"max_completion_tokens\": max_tokens}\n",
        "    else:\n",
        "        kwargs = {\"max_tokens\": max_tokens}\n",
        "\n",
        "    loop = asyncio.get_running_loop()\n",
        "    resp = await loop.run_in_executor(None, lambda: openai_client.chat.completions.create(\n",
        "        model=model_name, messages=messages, **kwargs\n",
        "    ))\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "MODEL_DISPATCH = {\n",
        "    \"gemini_flash\":      lambda p, i, m: call_gemini_async(\"gemini-2.5-flash\", p, i, m),\n",
        "    \"gemini_flash_lite\": lambda p, i, m: call_gemini_async(\"gemini-2.5-flash-lite\", p, i, m),\n",
        "    \"gpt4o\":             lambda p, i, m: call_openai_async(\"gpt-4o\", p, i, m),\n",
        "    \"gpt5_mini\":        lambda p, i, m: call_openai_async(\"gpt-5-mini\", p, i, m),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exec_logic"
      },
      "outputs": [],
      "source": [
        "def encode_image(img_path):\n",
        "    try:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        buffered = BytesIO()\n",
        "        img.save(buffered, format=\"JPEG\")\n",
        "        return {\"mime\": \"image/jpeg\", \"base64\": base64.b64encode(buffered.getvalue()).decode(\"utf-8\")}\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to encode image {img_path}: {e}\")\n",
        "\n",
        "def resize_and_encode_image(img_path, scale=0.8):\n",
        "    try:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        new_size = (int(img.width * scale), int(img.height * scale))\n",
        "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
        "        buffered = BytesIO()\n",
        "        img.save(buffered, format=\"JPEG\")\n",
        "        return {\"mime\": \"image/jpeg\", \"base64\": base64.b64encode(buffered.getvalue()).decode(\"utf-8\")}\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Failed to resize {img_path}: {e}\")\n",
        "\n",
        "async def process_dataset(dataset_name, prompt_key, model_key, concurrency):\n",
        "    if dataset_name == \"spatial_mm_one_obj\":\n",
        "      json_path = SPATIAL_MM_ROOT / 'data' / 'spatial_mm_one_obj.json'\n",
        "    elif dataset_name == \"spatial_mm_two_obj\":\n",
        "      json_path = SPATIAL_MM_ROOT / 'data' / 'spatial_mm_two_obj.json'\n",
        "    elif dataset_name == \"multihop_reasoning_309\":\n",
        "      json_path = SPATIAL_MM_ROOT / 'data' / 'multihop_reasoning_309.json'\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    out_path = OUTPUT_DIR / f\"{dataset_name}_{prompt_key}_{model_key}.json\"\n",
        "\n",
        "    existing_results = []\n",
        "    if out_path.exists():\n",
        "        with open(out_path, 'r') as f:\n",
        "            existing_results = json.load(f)\n",
        "    existing_ids = {x['image_name'] for x in existing_results if x.get('raw_response')}\n",
        "\n",
        "    to_process = [d for d in data if d['image_name'] not in existing_ids]\n",
        "    if not to_process:\n",
        "        return\n",
        "\n",
        "    print(f\"Processing {len(to_process)} items for {model_key}...\")\n",
        "\n",
        "    semaphore = asyncio.Semaphore(concurrency)\n",
        "    model_fn = MODEL_DISPATCH[model_key]\n",
        "    prompt_tmpl = PROMPT_CONFIGS[prompt_key][\"template\"]\n",
        "    max_tok = PROMPT_CONFIGS[prompt_key][\"max_tokens\"]\n",
        "\n",
        "    def is_connection_error(err: Exception) -> bool:\n",
        "        msg = str(err).lower()\n",
        "        return (\n",
        "            \"connection reset by peer\" in msg\n",
        "            or \"connection aborted\" in msg\n",
        "            or \"remote end closed connection without response\" in msg\n",
        "            or \"timed out\" in msg\n",
        "            or \"temporarily unavailable\" in msg\n",
        "            or \"429\" in msg\n",
        "            or \"503\" in msg\n",
        "        )\n",
        "\n",
        "    async def worker(entry):\n",
        "        async with semaphore:\n",
        "            image_name = entry.get('image_name')\n",
        "            if not image_name:\n",
        "                print(f\"[WARN] Entry missing image_name, skipping\")\n",
        "                return None\n",
        "\n",
        "            img_path = COT_IMAGE_DIR / image_name\n",
        "            if not img_path.exists():\n",
        "                print(f\"[WARN] Image not found: {image_name}\")\n",
        "                return None\n",
        "\n",
        "            max_retries = 5\n",
        "            resize_scale = [1.0, 0.9, 0.8, 0.6, 0.5]\n",
        "            shrink_idx = 0\n",
        "\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    if shrink_idx == 0:\n",
        "                        image_b64 = encode_image(img_path)\n",
        "                    else:\n",
        "                        scale = resize_scale[min(shrink_idx, len(resize_scale) - 1)]\n",
        "                        image_b64 = resize_and_encode_image(img_path, scale=scale)\n",
        "\n",
        "                    prompt = prompt_tmpl.format(question=entry['question'])\n",
        "                    response_text = await model_fn(prompt, image_b64, max_tok)\n",
        "\n",
        "                    res_entry = dict(entry)\n",
        "                    res_entry['raw_response'] = response_text\n",
        "                    return res_entry\n",
        "\n",
        "                except Exception as e:\n",
        "                    conn_err = is_connection_error(e)\n",
        "                    if conn_err:\n",
        "                        if attempt < max_retries - 1:\n",
        "                            wait_time = 2 * (attempt + 1)\n",
        "                            await asyncio.sleep(wait_time)\n",
        "                            continue\n",
        "                        else:\n",
        "                            print(f\"[FAIL] {image_name} {e}\")\n",
        "                            return None\n",
        "\n",
        "                    print(f\"[FAIL] {image_name} {type(e).__name__}: {e}\")\n",
        "                    return None\n",
        "            return None\n",
        "\n",
        "    results = list(existing_results)\n",
        "    total = len(to_process)\n",
        "    print(f\"Launching processing for {total} images with up to {concurrency} parallel calls...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    tasks = [asyncio.create_task(worker(entry)) for entry in to_process]\n",
        "    processed_count = 0\n",
        "    success_count = 0\n",
        "\n",
        "    for task in tqdm(asyncio.as_completed(tasks), total=total):\n",
        "        res = await task\n",
        "        if res is not None:\n",
        "            results.append(res)\n",
        "            success_count += 1\n",
        "            if success_count % 10 == 0:\n",
        "                with open(out_path, 'w') as f:\n",
        "                    json.dump(results, f, indent=4)\n",
        "\n",
        "        processed_count += 1\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed = end_time - start_time\n",
        "    failed_count = total - success_count\n",
        "\n",
        "    with open(out_path, 'w') as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "\n",
        "    print(f\"\\n Finished {model_key} \")\n",
        "    print(f\"  Time: {elapsed:.2f}s | Saved to: {out_path.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_CONFIGS = {\n",
        "\n",
        "    \"spatial_obj_baseline\": {\n",
        "        \"template\": (\n",
        "            \"Given an image, answer the multiple choice question.\\n\\n\"\n",
        "            \"{question}\\n\\n\"\n",
        "            \"Only answer by copying exactly the correct line from the choices above.\"\n",
        "        ),\n",
        "        \"max_tokens\": 1000,\n",
        "    },\n",
        "\n",
        "\n",
        "    \"spatial_obj_zeroshot_cot\": {\n",
        "        \"template\": (\n",
        "            \"Given an image, answer the multiple choice question.\\n\\n\"\n",
        "            \"{question}\\n\\n\"\n",
        "            \"Letâ€™s think step by step before answering the question. \"\n",
        "            \"Only answer by copying exactly the correct line from the choices above.\"\n",
        "        ),\n",
        "        \"max_tokens\": 1000,\n",
        "    },\n",
        "\n",
        "    \"multihop_baseline\": {\n",
        "        \"template\": (\n",
        "            \"Given an image, answer the question.\\n\\n\"\n",
        "            \"{question}\\n\\n\"\n",
        "            \"Answer the question in the format:\\n\"\n",
        "            \"Answer: <your answer here>\"\n",
        "        ),\n",
        "        \"max_tokens\": 1000,\n",
        "    },\n",
        "    \"multihop_zeroshot_cot\": {\n",
        "        \"template\": (\n",
        "            \"Given an image, answer the question about spatial relationship.\\n\\n\"\n",
        "            \"{question}\\n\\n\"\n",
        "            \"Let's think step by step before answering the question. \"\n",
        "            \"Explain your reasoning in less than three sentences.\\n\"\n",
        "            \"Then answer the question in the format:\\n\"\n",
        "            \"Answer: <your answer here>\"\n",
        "        ),\n",
        "        \"max_tokens\": 1000,\n",
        "    },\n",
        "}\n"
      ],
      "metadata": {
        "id": "7yTJhNudyXnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one_obj\n",
        "\n",
        "running = [\n",
        "    (\"gpt4o\", 1),\n",
        "    (\"gpt5_mini\", 5),\n",
        "    (\"gemini_flash\", 15)\n",
        "    (\"gemini_flash_lite\", 10)\n",
        "]\n",
        "\n",
        "prompts = [\"spatial_obj_baseline\", \"spatial_obj_zeroshot_cot\"]\n",
        "\n",
        "async def main_runner():\n",
        "    for model, limit in running:\n",
        "        for prompt in prompts:\n",
        "            print(f\"\\n>>> Starting {model} with {prompt}\")\n",
        "            await process_dataset(\"spatial_mm_one_obj\", prompt, model, concurrency=limit)\n",
        "\n",
        "await main_runner()\n"
      ],
      "metadata": {
        "id": "FS_RGath3CzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# two_obj\n",
        "\n",
        "running = [\n",
        "    (\"gpt4o\", 1),\n",
        "    (\"gpt5_mini\", 5),\n",
        "    (\"gemini_flash\", 15)\n",
        "    (\"gemini_flash_lite\", 10)\n",
        "]\n",
        "\n",
        "prompts = [\"spatial_obj_baseline\", \"spatial_obj_zeroshot_cot\"]\n",
        "\n",
        "async def main_runner():\n",
        "    for model, limit in running:\n",
        "        for prompt in prompts:\n",
        "            print(f\"\\n>>> Starting {model} with {prompt}\")\n",
        "            await process_dataset(\"spatial_mm_two_obj\", prompt, model, concurrency=limit)\n",
        "\n",
        "await main_runner()\n"
      ],
      "metadata": {
        "id": "Pfep3QKQ3yHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_exps"
      },
      "outputs": [],
      "source": [
        "# multihop\n",
        "\n",
        "running = [\n",
        "    (\"gpt4o\", 1),\n",
        "    (\"gpt5_mini\", 5),\n",
        "    (\"gemini_flash\", 15)\n",
        "    (\"gemini_flash_lite\", 10)\n",
        "]\n",
        "\n",
        "prompts = [\"multihop_baseline\", \"multihop_zeroshot_cot\"]\n",
        "\n",
        "async def main_runner():\n",
        "    for model, limit in running:\n",
        "        for prompt in prompts:\n",
        "            print(f\"\\n>>> Starting {model} with {prompt}\")\n",
        "            await process_dataset(\"multihop_reasoning_309\", prompt, model, concurrency=limit)\n",
        "\n",
        "await main_runner()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}